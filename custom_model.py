# -*- coding: utf-8 -*-
"""Custom_model_ds.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pnrBdoa_577XOyeVtRA-tkDNwe2zsDPB
"""

import os
import numpy as np
import pandas as pd
import cv2
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow as tf
from tensorflow.keras import layers, models, regularizers
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications import DenseNet121

# === CONFIG ===
IMG_SIZE = (224, 224)
BATCH_SIZE = 16
SEED = 42

# === STEP 1: DOWNLOAD AND EXTRACT DATA FROM KAGGLE ===
!pip install -q kaggle
from google.colab import files
uploaded = files.upload()
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d anasmohammedtahir/covidqu
!unzip -q covidqu.zip -d covidqu_data
!rm covidqu.zip

# === STEP 2: VERIFY AND BUILD DATAFRAME ===
base_path = 'covidqu_data'
lung_base = os.path.join(base_path, 'Lung Segmentation Data', 'Lung Segmentation Data')
infection_base = os.path.join(base_path, 'Infection Segmentation Data', 'Infection Segmentation Data')

def build_verified_dataset():
    samples = []
    for subset in ['Train', 'Test', 'Val']:
        for class_name in ['COVID-19', 'Non-COVID', 'Normal']:
            img_dir = os.path.join(lung_base, subset, class_name, 'images')
            lung_mask_path = os.path.join(lung_base, subset, class_name, 'lung masks')
            infection_mask_path = os.path.join(infection_base, subset, class_name, 'infection masks')
            if not os.path.exists(img_dir):
                continue
            for img_file in os.listdir(img_dir):
                base = os.path.splitext(img_file)[0]
                paths = {
                    'original_path': os.path.join(img_dir, img_file),
                    'lung_mask_path': os.path.join(lung_mask_path, base + '.png'),
                    'infection_mask_path': os.path.join(infection_mask_path, base + '.png')
                }
                if all(os.path.exists(p) for p in paths.values()):
                    samples.append({
                        'subset': subset,
                        'class': class_name,
                        **paths
                    })
    return pd.DataFrame(samples)

verified_df = build_verified_dataset()

def calculate_severity(row):
    lung_mask = cv2.imread(row['lung_mask_path'], cv2.IMREAD_GRAYSCALE)
    infection_mask = cv2.imread(row['infection_mask_path'], cv2.IMREAD_GRAYSCALE)
    lung_area = np.sum(lung_mask > 127)
    infection_area = np.sum(infection_mask > 127)
    return (infection_area / lung_area) * 100 if lung_area > 0 else 0

verified_df['severity'] = verified_df.apply(calculate_severity, axis=1)
verified_df = verified_df.dropna(subset=['severity'])

# === STEP 4: FILTER COVID-19 ONLY ===
covid_df = verified_df[verified_df['class'] == 'COVID-19'].copy()

# === STEP 5: TRAIN-VAL-TEST SPLIT ===
train_df = covid_df[covid_df['subset'] == 'Train']
val_df = covid_df[covid_df['subset'] == 'Val']
test_df = covid_df[covid_df['subset'] == 'Test']

# === STEP 6: IMAGE GENERATORS ===
def dual_input_generator(df, batch_size=BATCH_SIZE, shuffle=True):
    def gen():
        while True:
            if shuffle:
                df_shuffled = df.sample(frac=1).reset_index(drop=True)
            else:
                df_shuffled = df
            for i in range(0, len(df_shuffled), batch_size):
                batch_df = df_shuffled.iloc[i:i+batch_size]
                imgs, masks, labels = [], [], []
                for _, row in batch_df.iterrows():
                    img = cv2.resize(cv2.imread(row['original_path']), IMG_SIZE)
                    mask = cv2.resize(cv2.imread(row['infection_mask_path'], 0), IMG_SIZE)
                    imgs.append(img / 255.0)
                    masks.append(np.expand_dims(mask / 255.0, axis=-1))
                    labels.append(row['severity'])
                yield (np.array(imgs, dtype=np.float32), np.array(masks, dtype=np.float32)), np.array(labels, dtype=np.float32)
    return gen

steps_per_epoch = len(train_df) // BATCH_SIZE
val_steps = len(val_df) // BATCH_SIZE

train_ds = tf.data.Dataset.from_generator(
    dual_input_generator(train_df),
    output_signature=(
        (
            tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),
            tf.TensorSpec(shape=(None, 224, 224, 1), dtype=tf.float32)
        ),
        tf.TensorSpec(shape=(None,), dtype=tf.float32)
    )
)

val_ds = tf.data.Dataset.from_generator(
    dual_input_generator(val_df, shuffle=False),
    output_signature=(
        (
            tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),
            tf.TensorSpec(shape=(None, 224, 224, 1), dtype=tf.float32)
        ),
        tf.TensorSpec(shape=(None,), dtype=tf.float32)
    )
)

test_ds = tf.data.Dataset.from_generator(
    dual_input_generator(test_df, shuffle=False),
    output_signature=(
        (
            tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),
            tf.TensorSpec(shape=(None, 224, 224, 1), dtype=tf.float32)
        ),
        tf.TensorSpec(shape=(None,), dtype=tf.float32)
    )
)

# === STEP 7: CUSTOM MODEL ===
def build_dual_input_model(input_shape=(224, 224, 3)):
    img_input = tf.keras.Input(shape=input_shape, name='xray_image')
    mask_input = tf.keras.Input(shape=(224, 224, 1), name='infection_mask')

    base_model = DenseNet121(include_top=False, weights='imagenet', input_tensor=img_input)
    for layer in base_model.layers:
        layer.trainable = True

    x = base_model.output
    m = layers.Resizing(x.shape[1], x.shape[2])(mask_input)
    m = layers.Conv2D(x.shape[-1], 1, activation='sigmoid')(m)

    x = layers.Multiply()([x, m])
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(256, activation='relu')(x)
    x = layers.Dropout(0.4)(x)
    x = layers.Dense(128, activation='relu')(x)
    output = layers.Dense(1, name='severity')(x)

    model = tf.keras.Model(inputs=[img_input, mask_input], outputs=output)
    model.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='mse', metrics=['mae'])
    return model

# === STEP 8: TRAINING ===
model = build_dual_input_model()
model.summary()

checkpoint_path = '/content/drive/MyDrive/custom_dual_input_model.keras'
callbacks = [
    tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor='val_mae'),
    tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_best_only=True, monitor='val_mae'),
    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=4)
]
history = model.fit(
    train_ds,
    steps_per_epoch=steps_per_epoch,
    validation_data=val_ds,
    validation_steps=val_steps,
    epochs=50,
    callbacks=callbacks
)

# === STEP 6: EVALUATION ===
test_steps = len(test_df) // BATCH_SIZE
loss, mae = model.evaluate(test_ds, steps=test_steps)
print(f"Test MAE: {mae:.2f}%")

# === STEP 10: VISUALIZATION ===
test_gen_eval = dual_input_generator(test_df, shuffle=False)()
x_samples, y_true = next(test_gen_eval)
y_pred = model.predict((x_samples[0], x_samples[1])).flatten()

plt.figure(figsize=(8, 6))
plt.scatter(y_true, y_pred, alpha=0.5)
plt.plot([0, 100], [0, 100], 'r--')
plt.xlabel('True Severity (%)')
plt.ylabel('Predicted Severity (%)')
plt.title(f'Severity Predictions (MAE: {mae:.2f}%)')
plt.show()