# -*- coding: utf-8 -*-
"""Project-ds2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VfDiy26aAPqEKoYBNBrslBhz57lK_0Uj
"""

# Step 1: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Step 2: Set your save/load paths
import os

SAVE_DIR = '/content/drive/MyDrive/covid_project_checkpoints'
os.makedirs(SAVE_DIR, exist_ok=True)

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
from collections import Counter

# %% [markdown]
### COVID-19 Chest X-Ray EDA
# **Important Insight**: This dataset lacks severity labels (only binary COVID/non-COVID). Severity scoring requires additional datasets with clinical annotations.
# Steps:
# 1. Download dataset from Kaggle
# 2. Analyze class distribution
# 3. Inspect image characteristics
# 4. Visualize samples
# 5. Discuss severity scoring strategy

# %% [code]
# Install Kaggle API
!pip install -q kaggle
from google.colab import files

# Upload Kaggle API key (download from https://www.kaggle.com/settings)
uploaded = files.upload()
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# %% [code]
# Download and unzip dataset
!kaggle datasets download -d anasmohammedtahir/covidqu
!unzip -q covidqu.zip -d covidqu_data
!rm covidqu.zip

# %% [code]
# Imports
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
from collections import Counter


#    ```



# Based on the actual structure, let's modify our paths
import os

# Check what's inside the main directory
base_path = 'covidqu_data'
print("Root directory contents:", os.listdir(base_path))

# Base paths
base_path = 'covidqu_data'
lung_base = os.path.join(base_path, 'Lung Segmentation Data', 'Lung Segmentation Data')
infection_base = os.path.join(base_path, 'Infection Segmentation Data', 'Infection Segmentation Data')

# %% [code]
# 1. Analyze dataset structure
def analyze_structure(path):
    print(f"\nStructure of {path}:")
    for root, dirs, files in os.walk(path):
        level = root.replace(path, '').count(os.sep)
        indent = ' ' * 4 * level
        print(f"{indent}{os.path.basename(root)}/")
        subindent = ' ' * 4 * (level + 1)
        for f in files[:5]:  # Print first 5 files only
            print(f"{subindent}{f}")
        if len(files) > 5:
            print(f"{subindent}...{len(files)-5} more files")

analyze_structure(lung_base)
analyze_structure(infection_base)

# %% [code]
# 2. Create dataframe with all image paths and labels
def build_dataset_df(base_path):
    data = []
    for subset in ['Train', 'Test', 'Val']:
        for class_name in ['COVID-19', 'Non-COVID', 'Normal']:
            # Original images
            img_dir = os.path.join(base_path, subset, class_name, 'images')
            if os.path.exists(img_dir):
                for img_file in os.listdir(img_dir):
                    if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):
                        data.append({
                            'subset': subset,
                            'class': class_name,
                            'type': 'original',
                            'path': os.path.join(img_dir, img_file)
                        })

            # Lung masks
            lung_mask_dir = os.path.join(base_path, subset, class_name, 'lung masks')
            if os.path.exists(lung_mask_dir):
                for mask_file in os.listdir(lung_mask_dir):
                    if mask_file.lower().endswith(('.png', '.jpg', '.jpeg')):
                        data.append({
                            'subset': subset,
                            'class': class_name,
                            'type': 'lung_mask',
                            'path': os.path.join(lung_mask_dir, mask_file)
                        })

    return pd.DataFrame(data)

# Create dataframes for both segmentation types
lung_df = build_dataset_df(lung_base)
infection_df = build_dataset_df(infection_base)

print("\nLung Data Summary:")
print(lung_df.groupby(['subset', 'class', 'type']).size().unstack())

print("\nInfection Data Summary:")
print(infection_df.groupby(['subset', 'class', 'type']).size().unstack())

# %% [code]
# 3. Merge the dataframes to create complete samples
# First ensure we can match files between the two datasets
def extract_base_filename(path):
    """Extract comparable filename from path"""
    filename = os.path.basename(path)
    # Remove possible prefixes/suffixes
    for s in ['_lung_mask', '_infection', '_mask']:
        filename = filename.replace(s, '')
    return filename

# Create matching keys
lung_df['match_key'] = lung_df['path'].apply(extract_base_filename)
infection_df['match_key'] = infection_df['path'].apply(extract_base_filename)

# Merge the dataframes
merged_df = pd.merge(
    lung_df[lung_df['type'] == 'original'],
    lung_df[lung_df['type'] == 'lung_mask'],
    on=['subset', 'class', 'match_key'],
    suffixes=('_original', '_lung_mask')
)

merged_df = pd.merge(
    merged_df,
    infection_df[infection_df['type'] == 'lung_mask'],
    on=['subset', 'class', 'match_key']
)

# Clean up column names
merged_df = merged_df.rename(columns={
    'path': 'infection_mask_path',
    'type': 'infection_mask_type'
})
merged_df = merged_df[[
    'subset', 'class', 'match_key',
    'path_original', 'path_lung_mask', 'infection_mask_path'
]]

print("\nMerged dataset samples:")
print(merged_df.head())

# %% [code]
# 4. Calculate severity scores
def calculate_severity(row):
    try:
        lung_mask = cv2.imread(row['path_lung_mask'], cv2.IMREAD_GRAYSCALE)
        infection_mask = cv2.imread(row['infection_mask_path'], cv2.IMREAD_GRAYSCALE)

        if lung_mask is None or infection_mask is None:
            return np.nan

        # Calculate areas
        lung_area = np.sum(lung_mask > 127)
        infection_area = np.sum(infection_mask > 127)

        return (infection_area / lung_area) * 100 if lung_area > 0 else 0
    except:
        return np.nan

merged_df['severity'] = merged_df.apply(calculate_severity, axis=1)

# Remove rows with missing severity scores
merged_df = merged_df.dropna(subset=['severity'])

print("\nSeverity statistics by class:")
print(merged_df.groupby('class')['severity'].describe())

# %% [code]
# 5. Visualize samples
def visualize_sample(row):
    plt.figure(figsize=(15, 5))

    # Original image
    plt.subplot(1, 4, 1)
    img = cv2.imread(row['path_original'])
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.imshow(img)
    plt.title(f"Original\n{row['class']}")  # Fixed: using row['class'] instead of undefined class
    plt.axis('off')

    # Lung mask
    plt.subplot(1, 4, 2)
    lung_mask = cv2.imread(row['path_lung_mask'], cv2.IMREAD_GRAYSCALE)
    plt.imshow(lung_mask, cmap='gray')
    plt.title("Lung Mask")
    plt.axis('off')

    # Infection mask
    plt.subplot(1, 4, 3)
    infection_mask = cv2.imread(row['infection_mask_path'], cv2.IMREAD_GRAYSCALE)
    plt.imshow(infection_mask, cmap='hot')
    plt.title("Infection Mask")
    plt.axis('off')

    # Combined
    plt.subplot(1, 4, 4)
    overlay = cv2.addWeighted(cv2.cvtColor(img, cv2.COLOR_RGB2GRAY), 0.7, infection_mask, 0.3, 0)
    plt.imshow(overlay, cmap='viridis')
    plt.title(f"Severity: {row['severity']:.2f}%")
    plt.axis('off')

    plt.tight_layout()
    plt.show()

# Visualize random samples for each class
for class_name in merged_df['class'].unique():
    print(f"\nVisualizing random {class_name} sample:")
    sample = merged_df[merged_df['class'] == class_name].sample(1).iloc[0]
    visualize_sample(sample)



### Next Steps
def preprocess_image(img_path, target_size=(512, 512)):
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = cv2.resize(img, target_size)
    return img / 255.0  # Normalize

import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Set root paths
lung_root = 'covidqu_data/Lung Segmentation Data/Lung Segmentation Data'
infection_root = 'covidqu_data/Infection Segmentation Data/Infection Segmentation Data'

# Function to count files per class/category
def count_files(root_dir, has_infection_masks=False):
    records = []
    for split in ['Train', 'Val', 'Test']:
        for cls in ['Normal', 'Non-COVID', 'COVID-19']:
            image_dir = os.path.join(root_dir, split, cls, 'images')
            lung_mask_dir = os.path.join(root_dir, split, cls, 'lung masks')
            inf_mask_dir = os.path.join(root_dir, split, cls, 'infection masks') if has_infection_masks else None

            image_count = len(os.listdir(image_dir)) if os.path.exists(image_dir) else 0
            lung_mask_count = len(os.listdir(lung_mask_dir)) if os.path.exists(lung_mask_dir) else 0
            inf_mask_count = len(os.listdir(inf_mask_dir)) if inf_mask_dir and os.path.exists(inf_mask_dir) else None

            records.append({
                'Dataset': 'Infection' if has_infection_masks else 'Lung',
                'Split': split,
                'Class': cls,
                'Images': image_count,
                'Lung Masks': lung_mask_count,
                'Infection Masks': inf_mask_count
            })
    return pd.DataFrame(records)

# Get stats
df_lung = count_files(lung_root)
df_infection = count_files(infection_root, has_infection_masks=True)

# Combine both for joint analysis
df_all = pd.concat([df_lung, df_infection], ignore_index=True)

# Display overview
print(df_all)

# Save to CSV for reference
df_all.to_csv('eda_image_statistics.csv', index=False)

plt.figure(figsize=(12, 6))
sns.barplot(data=df_all, x='Split', y='Images', hue='Class', ci=None)
plt.title('Image Count per Class per Split')
plt.ylabel('Number of Images')
plt.xlabel('Dataset Split')
plt.legend(title='Class')
plt.tight_layout()
plt.show()

melted = df_all.melt(id_vars=['Dataset', 'Split', 'Class'], value_vars=['Lung Masks', 'Infection Masks'])
plt.figure(figsize=(14, 6))
sns.barplot(data=melted, x='Split', y='value', hue='variable')
plt.title('Mask Availability per Split')
plt.ylabel('Number of Masks')
plt.xlabel('Split')
plt.legend(title='Mask Type')
plt.tight_layout()
plt.show()

for dataset in ['Lung', 'Infection']:
    subset = df_all[df_all['Dataset'] == dataset]
    class_counts = subset.groupby('Class')['Images'].sum()
    plt.figure()
    plt.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%')
    plt.title(f'{dataset} Dataset - Class Distribution')
    plt.show()

import cv2
import numpy as np

def visualize_sample(image_path, lung_mask_path=None, infection_mask_path=None):
    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    plt.figure(figsize=(15, 5))

    plt.subplot(1, 3, 1)
    plt.imshow(image, cmap='gray')
    plt.title('Image')

    if lung_mask_path:
        lung_mask = cv2.imread(lung_mask_path, cv2.IMREAD_GRAYSCALE)
        plt.subplot(1, 3, 2)
        plt.imshow(lung_mask, cmap='gray')
        plt.title('Lung Mask')

    if infection_mask_path:
        infection_mask = cv2.imread(infection_mask_path, cv2.IMREAD_GRAYSCALE)
        plt.subplot(1, 3, 3)
        plt.imshow(infection_mask, cmap='gray')
        plt.title('Infection Mask')

    plt.tight_layout()
    plt.show()

# Example usage (adjust the paths):
visualize_sample(
    'covidqu_data/Infection Segmentation Data/Infection Segmentation Data/Train/COVID-19/images/covid_4017.png',
    'covidqu_data/Infection Segmentation Data/Infection Segmentation Data/Train/COVID-19/lung masks/covid_4017.png',
    'covidqu_data/Infection Segmentation Data/Infection Segmentation Data/Train/COVID-19/infection masks/covid_4017.png'
)

# Quick path verification test
test_path = 'covidqu_data/Infection Segmentation Data/Infection Segmentation Data/Train/COVID-19/images'
print(f"Test path exists: {os.path.exists(test_path)}")
print(f"Contents of COVID-19 train images: {os.listdir(test_path)[:5] if os.path.exists(test_path) else 'Path not found'}")

import os
import cv2
import pandas as pd
from tqdm import tqdm

# Define paths
base_path = 'covidqu_data'
lung_base = os.path.join(base_path, 'Lung Segmentation Data', 'Lung Segmentation Data')
infection_base = os.path.join(base_path, 'Infection Segmentation Data', 'Infection Segmentation Data')

def build_verified_dataset():
    samples = []

    # Iterate through all subsets and classes
    for subset in ['Train', 'Test', 'Val']:
        for class_name in ['COVID-19', 'Non-COVID', 'Normal']:
            # Path to original images
            img_dir = os.path.join(lung_base, subset, class_name, 'images')

            # Skip if directory doesn't exist
            if not os.path.exists(img_dir):
                print(f"Warning: Missing directory - {img_dir}")
                continue

            # Get all image files
            image_files = [f for f in os.listdir(img_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

            # Track found/missing files
            found = 0
            missing = 0

            for img_file in tqdm(image_files, desc=f"{subset}/{class_name}"):
                base_name = os.path.splitext(img_file)[0]

                # Construct all required paths
                original_path = os.path.join(img_dir, img_file)
                lung_mask_path = os.path.join(lung_base, subset, class_name, 'lung masks', f"{base_name}.png")
                infection_mask_path = os.path.join(infection_base, subset, class_name, 'infection masks', f"{base_name}.png")

                # Verify all files exist
                if all(os.path.exists(p) for p in [original_path, lung_mask_path, infection_mask_path]):
                    samples.append({
                        'subset': subset,
                        'class': class_name,
                        'original_path': original_path,
                        'lung_mask_path': lung_mask_path,
                        'infection_mask_path': infection_mask_path
                    })
                    found += 1
                else:
                    missing += 1

            print(f"{subset}/{class_name}: Found {found} complete samples, {missing} incomplete")

    return pd.DataFrame(samples)

print("Building verified dataset...")
verified_df = build_verified_dataset()

if len(verified_df) > 0:
    print(f"\nSuccessfully loaded {len(verified_df)} complete samples")
    print("\nSample records:")
    print(verified_df.head())

    # Save the verified dataset
    verified_df.to_pickle('verified_covid_dataset.pkl')
    print("\nSaved verified dataset to 'verified_covid_dataset.pkl'")

    # Show distribution
    print("\nClass distribution:")
    print(verified_df['class'].value_counts())

    print("\nSubset distribution:")
    print(verified_df['subset'].value_counts())
else:
    print("\nNo complete samples found. Please check the dataset structure.")

import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

# 1. Calculate severity scores
def calculate_severity(row):
    lung_mask = cv2.imread(row['lung_mask_path'], cv2.IMREAD_GRAYSCALE)
    infection_mask = cv2.imread(row['infection_mask_path'], cv2.IMREAD_GRAYSCALE)

    lung_area = np.sum(lung_mask > 127)
    infection_area = np.sum(infection_mask > 127)

    return (infection_area / lung_area) * 100 if lung_area > 0 else 0

print("Calculating severity scores...")
verified_df['severity'] = verified_df.apply(calculate_severity, axis=1)

# 2. Create severity categories
bins = [0, 10, 25, 50, 100]
labels = ['mild', 'moderate', 'severe', 'critical']
verified_df['severity_category'] = pd.cut(verified_df['severity'], bins=bins, labels=labels)

# 3. Show severity distribution
print("\nSeverity distribution:")
print(verified_df['severity_category'].value_counts())

# 4. Prepare data splits (maintaining original splits)
train_df = verified_df[verified_df['subset'] == 'Train']
val_df = verified_df[verified_df['subset'] == 'Val']
test_df = verified_df[verified_df['subset'] == 'Test']

# 5. Save processed datasets
train_df.to_pickle('train_dataset.pkl')
val_df.to_pickle('val_dataset.pkl')
test_df.to_pickle('test_dataset.pkl')

print("\nData preparation complete!")
print(f"Training samples: {len(train_df)}")
print(f"Validation samples: {len(val_df)}")
print(f"Test samples: {len(test_df)}")

import matplotlib.pyplot as plt
import seaborn as sns
import random

# Set style
sns.set_style('whitegrid')
plt.figure(figsize=(15, 10))

# 1. Severity distribution by class
plt.subplot(2, 2, 1)
sns.countplot(data=verified_df, x='severity_category', hue='class')
plt.title('Severity Distribution by Class')
plt.xlabel('Severity Category')
plt.ylabel('Count')

# 2. Severity score distribution
plt.subplot(2, 2, 2)
sns.histplot(verified_df['severity'], bins=30, kde=True)
plt.title('Severity Score Distribution')
plt.xlabel('Severity Percentage')

# 3. Class distribution across splits
plt.subplot(2, 2, 3)
sns.countplot(data=verified_df, x='subset', hue='class')
plt.title('Class Distribution Across Splits')
plt.xlabel('Dataset Split')
plt.ylabel('Count')

plt.tight_layout()
plt.show()

# 4. Visualize random samples with masks and severity
def visualize_random_samples(df, n_samples=3):
    fig, axes = plt.subplots(n_samples, 4, figsize=(20, 5*n_samples))

    for i in range(n_samples):
        sample = df.sample(1).iloc[0]

        # Load images
        img = cv2.cvtColor(cv2.imread(sample['original_path']), cv2.COLOR_BGR2RGB)
        lung_mask = cv2.imread(sample['lung_mask_path'], cv2.IMREAD_GRAYSCALE)
        infection_mask = cv2.imread(sample['infection_mask_path'], cv2.IMREAD_GRAYSCALE)

        # Original image
        axes[i, 0].imshow(img)
        axes[i, 0].set_title(f"Original\nClass: {sample['class']}")
        axes[i, 0].axis('off')

        # Lung mask
        axes[i, 1].imshow(lung_mask, cmap='gray')
        axes[i, 1].set_title("Lung Mask")
        axes[i, 1].axis('off')

        # Infection mask
        axes[i, 2].imshow(infection_mask, cmap='hot')
        axes[i, 2].set_title("Infection Mask")
        axes[i, 2].axis('off')

        # Combined
        overlay = cv2.addWeighted(cv2.cvtColor(img, cv2.COLOR_RGB2GRAY), 0.7, infection_mask, 0.3, 0)
        axes[i, 3].imshow(overlay, cmap='viridis')
        axes[i, 3].set_title(f"Severity: {sample['severity']:.1f}% ({sample['severity_category']})")
        axes[i, 3].axis('off')

    plt.tight_layout()
    plt.show()

print("\nVisualizing random samples...")
visualize_random_samples(verified_df)

from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow as tf

# Constants
IMG_SIZE = (224, 224)
BATCH_SIZE = 32

def create_data_generator(df, augment=False):
    if augment:
        datagen = ImageDataGenerator(
            rescale=1./255,
            rotation_range=15,
            width_shift_range=0.1,
            height_shift_range=0.1,
            shear_range=0.1,
            zoom_range=0.1,
            horizontal_flip=True,
            fill_mode='nearest'
        )
    else:
        datagen = ImageDataGenerator(rescale=1./255)

    generator = datagen.flow_from_dataframe(
        dataframe=df,
        x_col='original_path',
        y_col='severity_category',
        target_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        class_mode='categorical',
        shuffle=True
    )
    return generator

# Create generators
# Filter out rows where 'severity_category' is NaN before creating generators
train_df_filtered = train_df.dropna(subset=['severity_category'])
val_df_filtered = val_df.dropna(subset=['severity_category'])
test_df_filtered = test_df.dropna(subset=['severity_category'])


train_generator = create_data_generator(train_df_filtered, augment=True)
val_generator = create_data_generator(val_df_filtered)
test_generator = create_data_generator(test_df_filtered)

# Show class indices
print("\nClass indices:", train_generator.class_indices)

from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras import layers, models

def build_model(num_classes=4):
    base_model = EfficientNetB0(
        input_shape=(224, 224, 3),
        include_top=False,
        weights='imagenet'
    )

    # Freeze base model
    base_model.trainable = False

    # Add custom head
    inputs = tf.keras.Input(shape=(224, 224, 3))
    x = base_model(inputs, training=False)
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(128, activation='relu')(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)

    model = tf.keras.Model(inputs, outputs)

    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

model = build_model()
model.summary()

from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

callbacks = [
    EarlyStopping(patience=5, restore_best_weights=True),
    ModelCheckpoint('best_model.h5', save_best_only=True)
]

history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=30,
    callbacks=callbacks
)

from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.applications import EfficientNetB0

def build_improved_model(num_classes=4):
    # Load base model with pretrained weights
    base_model = EfficientNetB0(
        input_shape=(224, 224, 3),
        include_top=False,
        weights='imagenet'
    )

    # Partial unfreezing (last 10 layers)
    for layer in base_model.layers[:-10]:
        layer.trainable = False
    for layer in base_model.layers[-10:]:
        layer.trainable = True

    # Enhanced architecture
    inputs = tf.keras.Input(shape=(224, 224, 3))
    x = base_model(inputs, training=True)  # Enable training mode for BN layers
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(256, activation='relu')(x)
    x = layers.Dropout(0.5)(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)

    model = tf.keras.Model(inputs, outputs)

    # Custom learning rate and class weights
    model.compile(
        optimizer=optimizers.Adam(learning_rate=1e-4),
        loss='categorical_crossentropy',
        metrics=['accuracy',
                tf.keras.metrics.AUC(name='auc'),
                tf.keras.metrics.Precision(name='precision'),
                tf.keras.metrics.Recall(name='recall')]
    )

    return model

model = build_improved_model()
model.summary()

from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# Calculate class weights
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(train_generator.classes),
    y=train_generator.classes
)
class_weights = dict(enumerate(class_weights))
print("Class weights:", class_weights)

from tensorflow.keras.callbacks import (
    ModelCheckpoint,
    EarlyStopping,
    ReduceLROnPlateau
)

callbacks = [
    EarlyStopping(patience=10, restore_best_weights=True, monitor='val_auc'),
    ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_auc'),
    ReduceLROnPlateau(factor=0.5, patience=3)
]

history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=50,
    class_weight=class_weights,
    callbacks=callbacks
)

# Load best model
model = tf.keras.models.load_model('best_model.keras')

# Evaluate
test_results = model.evaluate(test_generator)
print(f"Test Accuracy: {test_results[1]:.2%}")
print(f"Test AUC: {test_results[2]:.2%}")

# Confusion matrix
from sklearn.metrics import confusion_matrix
import seaborn as sns

y_true = test_generator.classes
y_pred = np.argmax(model.predict(test_generator), axis=1)

cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d',
            xticklabels=labels,
            yticklabels=labels)
plt.title('Confusion Matrix')
plt.show()

from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import AUC

# 1. Prepare binary dataset
binary_df = verified_df.copy()
binary_df['is_covid'] = (binary_df['class'] == 'COVID-19').astype(int)

# 2. Create balanced generator
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=15,
    width_shift_range=0.1,
    horizontal_flip=True,
    validation_split=0.2
)

train_generator = train_datagen.flow_from_dataframe(
    dataframe=binary_df[binary_df['subset']=='Train'],
    x_col='original_path',
    y_col='is_covid',
    class_mode='raw',
    target_size=(224,224),
    batch_size=32,
    shuffle=True
)

# 3. Train DenseNet121 binary classifier
base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224,224,3))
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(1, activation='sigmoid')
])
model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy', AUC()])
model.fit(train_generator, epochs=15)

model.save('/content/drive/MyDrive/covid_binary_model.h5')

# 1. Filter COVID cases
covid_df = verified_df[verified_df['class']=='COVID-19'].copy()

# 2. Create severity generator
severity_generator = ImageDataGenerator(rescale=1./255).flow_from_dataframe(
    dataframe=covid_df,
    x_col='original_path',
    y_col='severity',  # Continuous value
    class_mode='raw',
    target_size=(224,224),
    batch_size=16
)

# 3. Train regression model
severity_model = Sequential([
    DenseNet121(weights='imagenet', include_top=False, input_shape=(224,224,3)),
    GlobalAveragePooling2D(),
    Dense(128, activation='relu'),
    Dense(1)  # Linear output for severity %
])
severity_model.compile(optimizer=Adam(1e-4), loss='mse', metrics=['mae'])
severity_model.fit(severity_generator, epochs=20)

severity_model.save('/content/drive/MyDrive/severity_regression_model.h5')

# Evaluate COVID detector
test_generator = train_datagen.flow_from_dataframe(
    dataframe=binary_df[binary_df['subset']=='Test'],
    x_col='original_path',
    y_col='is_covid',
    class_mode='raw',
    target_size=(224,224),
    batch_size=32,
    shuffle=False
)

loss, acc, auc = model.evaluate(test_generator)
print(f"Test Accuracy: {acc:.1%}, AUC: {auc:.1%}")

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Get true labels and predictions
test_generator.reset()  # Important for correct label order
y_true = test_generator.labels
y_pred = (model.predict(test_generator) > 0.5).astype(int).flatten()

# 2. Calculate metrics
accuracy = accuracy_score(y_true, y_pred)
print(f"Test Accuracy: {accuracy:.1%}")
print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=['Non-COVID', 'COVID']))

# 3. Confusion Matrix
plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_true, y_pred),
            annot=True, fmt='d', cmap='Blues',
            xticklabels=['Non-COVID', 'COVID'],
            yticklabels=['Non-COVID', 'COVID'])
plt.title(f'COVID Detection (Accuracy: {accuracy:.1%})')
plt.show()

import tensorflow as tf
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.applications import DenseNet121
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split # You might need this later for stratified splitting
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import os

# Assuming 'covid_df' is already loaded and contains 'original_path', 'subset', and 'severity_category'
# Make sure the 'severity_category' column exists and has the correct values (e.g., 'mild', 'moderate', 'severe', 'critical')
# Ensure 'covid_df' only contains rows for COVID-19 cases if this model is specifically for severity.

# Convert the 'severity_category' column to string type
covid_df['severity_category'] = covid_df['severity_category'].astype(str)

# Define constants
IMG_SIZE = (224, 224)
BATCH_SIZE = 16
NUM_CLASSES = len(covid_df['severity_category'].unique()) # Get the number of unique categories

# Data augmentation and preprocessing for training
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=15,
    zoom_range=0.1,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True
)

# Only rescaling for validation and testing
val_test_datagen = ImageDataGenerator(rescale=1./255)

# Create data generators
# Filter out rows where 'severity_category' is NaN
train_df_filtered = covid_df[(covid_df['subset'] == 'Train') & covid_df['severity_category'].notna()]
val_df_filtered = covid_df[(covid_df['subset'] == 'Val') & covid_df['severity_category'].notna()]
test_df_filtered = covid_df[(covid_df['subset'] == 'Test') & covid_df['severity_category'].notna()]


train_gen = train_datagen.flow_from_dataframe(
    dataframe=train_df_filtered,
    x_col='original_path',
    y_col='severity_category',
    class_mode='categorical', # Use categorical for string labels -> one-hot encoding
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    shuffle=True
)

val_gen = val_test_datagen.flow_from_dataframe(
    dataframe=val_df_filtered,
    x_col='original_path',
    y_col='severity_category',
    class_mode='categorical', # Use categorical for string labels -> one-hot encoding
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    shuffle=False
)

test_gen = val_test_datagen.flow_from_dataframe(
    dataframe=test_df_filtered,
    x_col='original_path',
    y_col='severity_category',
    class_mode='categorical', # Use categorical for string labels -> one-hot encoding
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    shuffle=False
)


# Get the class indices from the training generator to ensure consistency
# and map integer predictions back to category names.
class_indices = train_gen.class_indices
# Create a mapping from index to class name
idx_to_class = {v: k for k, v in class_indices.items()}
# Ensure the labels list matches the order of class indices for classification report
sorted_labels = [idx_to_class[i] for i in sorted(idx_to_class.keys())]


# Define the model architecture
def build_severity_model(num_classes):
    base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))
    # Keep the base model frozen for initial training, then unfreeze later if needed
    base_model.trainable = False

    model = models.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation='softmax')  # Output units equal to number of classes
    ])

    model.compile(
        optimizer=optimizers.Adam(1e-4),
        loss='categorical_crossentropy', # Use categorical_crossentropy for one-hot encoded labels
        metrics=['accuracy']
    )
    return model

# Build the model
severity_model = build_severity_model(NUM_CLASSES)

# Train the model
print("Starting model training...")
history = severity_model.fit(
    train_gen,
    steps_per_epoch=train_gen.samples // BATCH_SIZE, # Calculate steps per epoch
    validation_data=val_gen,
    validation_steps=val_gen.samples // BATCH_SIZE, # Calculate validation steps
    epochs=30,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_accuracy', restore_best_weights=True),
        tf.keras.callbacks.ModelCheckpoint('best_severity_model.keras', save_best_only=True, monitor='val_accuracy')
    ]
)
print("Model training finished.")

# Load the best model
# severity_model = tf.keras.models.load_model('best_severity_model.keras')

# Evaluate the model
print("\nEvaluating the model on the test set...")
# Reset test generator to ensure correct order for evaluation
test_gen.reset()
eval_results = severity_model.evaluate(test_gen, steps=test_gen.samples // BATCH_SIZE)

print(f"Test Loss: {eval_results[0]:.4f}")
print(f"Test Accuracy: {eval_results[1]:.2%}")

# Generate classification report and confusion matrix
print("\nGenerating Classification Report and Confusion Matrix...")
# Predict probabilities for the test set
test_gen.reset() # Reset again before prediction
y_pred_probs = severity_model.predict(test_gen, steps=test_gen.samples // BATCH_SIZE + 1) # Add 1 to steps to ensure all samples are included

# Get true labels and predicted labels (integer indices)
y_true_indices = test_gen.classes
y_pred_indices = np.argmax(y_pred_probs, axis=1)

# Trim predicted indices to match the number of true labels
# This is necessary because the last batch from the generator might be smaller
# than the batch size, and predict might return more predictions than samples if steps is not exact.
y_pred_indices = y_pred_indices[:len(y_true_indices)]


print("\nClassification Report:")
print(classification_report(y_true_indices, y_pred_indices, target_names=sorted_labels))

# Confusion Matrix
plt.figure(figsize=(10, 8))
sns.heatmap(confusion_matrix(y_true_indices, y_pred_indices),
            annot=True, fmt='d',
            xticklabels=sorted_labels,
            yticklabels=sorted_labels,
            cmap='Blues')
plt.title('Severity Prediction Confusion Matrix')
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.show()

checkpoint_path = '/content/drive/MyDrive/best_severity_model.keras'

callbacks=[
    tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_accuracy', restore_best_weights=True),
    tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_best_only=True, monitor='val_accuracy')
]

history = severity_model.fit(
    train_gen,
    steps_per_epoch=train_gen.samples // BATCH_SIZE,
    validation_data=val_gen,
    validation_steps=val_gen.samples // BATCH_SIZE,
    epochs=30,
    callbacks=callbacks
)

import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import MeanAbsoluteError # Import MAE if used as a metric
from tensorflow.keras.losses import MeanSquaredError # Import MSE

# Define custom objects for loading
custom_objects = {
    'mse': MeanSquaredError(),
    'mae': MeanAbsoluteError()
}


# Load the binary classification model from Google Drive
try:
    binary_model_path = '/content/drive/MyDrive/covid_binary_model.h5'
    binary_model = tf.keras.models.load_model(binary_model_path)
    print(f"Successfully loaded binary model from {binary_model_path}")
    binary_model.summary()
except Exception as e:
    print(f"Error loading binary model: {e}")
    binary_model = None

# Load the severity regression model from Google Drive
try:
    severity_model_path = '/content/drive/MyDrive/severity_regression_model.h5'
    # Load the model with custom objects
    severity_model = tf.keras.models.load_model(severity_model_path, custom_objects=custom_objects)
    print(f"Successfully loaded severity regression model from {severity_model_path}")
    # Recompile the model to ensure the optimizer and metrics are set
    severity_model.compile(optimizer=Adam(1e-4), loss='mse', metrics=['mae'])
    print("Severity regression model recompiled successfully.")
    severity_model.summary()
except Exception as e:
    print(f"Error loading severity regression model: {e}")
    severity_model = None

# Load the best severity classification model from Google Drive
try:
    best_severity_model_path = '/content/drive/MyDrive/best_severity_model.keras'
    # Loading a .keras model does not typically require custom_objects for standard layers/losses
    best_severity_model = tf.keras.models.load_model(best_severity_model_path)
    print(f"Successfully loaded best severity classification model from {best_severity_model_path}")
    best_severity_model.summary()
except Exception as e:
    print(f"Error loading best severity classification model: {e}")
    best_severity_model = None

if severity_model is not None:
    # Prepare COVID-only test data for severity regression
    covid_test_df = verified_df[(verified_df['subset'] == 'Test') &
                               (verified_df['class'] == 'COVID-19')].copy()

    severity_test_gen = ImageDataGenerator(rescale=1./255).flow_from_dataframe(
        dataframe=covid_test_df,
        x_col='original_path',
        y_col='severity',
        class_mode='raw',
        target_size=(224, 224),
        batch_size=16,
        shuffle=False
    )

    # Evaluate
    print("Evaluating severity regression model...")
    loss, mae = severity_model.evaluate(severity_test_gen)
    print(f"Test MSE: {loss:.4f}, MAE: {mae:.4f}")

    # Predictions vs. Ground Truth
    severity_test_gen.reset()
    y_pred = severity_model.predict(severity_test_gen).flatten()
    y_true = severity_test_gen.labels

    # Plot predictions
    plt.figure(figsize=(10, 6))
    plt.scatter(y_true, y_pred, alpha=0.5)
    plt.plot([0, 100], [0, 100], 'r--')  # Perfect prediction line
    plt.xlabel("True Severity (%)")
    plt.ylabel("Predicted Severity (%)")
    plt.title(f"Severity Regression (MAE: {mae:.2f}%)")
    plt.show()
else:
    print("Severity regression model not loaded.")

if best_severity_model is not None:
    # Prepare COVID-only test data for severity classification
    covid_test_df = verified_df[(verified_df['subset'] == 'Test') &
                               (verified_df['class'] == 'COVID-19')].copy()

    severity_class_test_gen = ImageDataGenerator(rescale=1./255).flow_from_dataframe(
        dataframe=covid_test_df,
        x_col='original_path',
        y_col='severity_category',
        class_mode='categorical',
        target_size=(224, 224),
        batch_size=16,
        shuffle=False
    )

    # Evaluate
    print("Evaluating severity classification model...")
    loss, accuracy = best_severity_model.evaluate(severity_class_test_gen)
    print(f"Test Accuracy: {accuracy:.2%}")

    # Confusion Matrix
    severity_class_test_gen.reset()
    y_pred = np.argmax(best_severity_model.predict(severity_class_test_gen), axis=1)
    y_true = severity_class_test_gen.classes

    # Map indices to labels
    class_labels = list(severity_class_test_gen.class_indices.keys())

    plt.figure(figsize=(8, 6))
    sns.heatmap(confusion_matrix(y_true, y_pred),
                annot=True, fmt='d',
                xticklabels=class_labels,
                yticklabels=class_labels,
                cmap='Blues')
    plt.title(f"Severity Classification (Accuracy: {accuracy:.2%})")
    plt.show()
else:
    print("Severity classification model not loaded.")

from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras import layers, models

def build_new_severity_model():
    base_model = EfficientNetB0(
        input_shape=(224, 224, 3),
        include_top=False,
        weights='imagenet'
    )
    base_model.trainable = False  # Freeze for initial training

    model = models.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.3),
        layers.Dense(1)  # Regression output
    ])

    model.compile(
        optimizer='adam',
        loss='mse',
        metrics=['mae']
    )
    return model

# Ensure the DataFrame has the required columns
if 'verified_df' in locals():
    # Filter COVID-19 cases and ensure 'original_path' exists
    covid_df = verified_df[verified_df['class'] == 'COVID-19'].copy()

    # Verify columns (debugging)
    print("Columns in covid_df:", covid_df.columns.tolist())

    if 'original_path' not in covid_df.columns:
        # Try alternative column names (adjust based on your actual DataFrame)
        if 'path_original' in covid_df.columns:
            covid_df['original_path'] = covid_df['path_original']
        else:
            raise KeyError("Neither 'original_path' nor 'path_original' found in DataFrame.")

    # Split data
    train_df = covid_df[covid_df['subset'] == 'Train']
    val_df = covid_df[covid_df['subset'] == 'Val']

    # Create generators
    train_gen = ImageDataGenerator(rescale=1./255).flow_from_dataframe(
        dataframe=train_df,
        x_col='original_path',  # Now guaranteed to exist
        y_col='severity',
        class_mode='raw',
        target_size=(224, 224),
        batch_size=16
    )

    val_gen = ImageDataGenerator(rescale=1./255).flow_from_dataframe(
        dataframe=val_df,
        x_col='original_path',
        y_col='severity',
        class_mode='raw',
        target_size=(224, 224),
        batch_size=16
    )

    # Train the model
    new_severity_model = build_new_severity_model()
    history = new_severity_model.fit(
        train_gen,
        validation_data=val_gen,
        epochs=20,
        callbacks=[
            tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)
        ]
    )

    # Save the model
    new_severity_model.save('/content/drive/MyDrive/new_severity_model.keras')
    print("New severity model trained and saved.")
else:
    print("Error: 'verified_df' not found. Load your dataset first.")

# Evaluate the new model
test_gen = ImageDataGenerator(rescale=1./255).flow_from_dataframe(
    dataframe=covid_test_df[covid_test_df['subset'] == 'Test'],
    x_col='original_path',
    y_col='severity',
    class_mode='raw',
    target_size=(224, 224),
    batch_size=16
)

loss, mae = new_severity_model.evaluate(test_gen)
print(f"New Model Test MAE: {mae:.2f}%")

# Compare with old regression model (if available)
if severity_model:
    old_mae = severity_model.evaluate(test_gen)[1]
    print(f"Old Model Test MAE: {old_mae:.2f}%")

# Check for data leaks or incorrect preprocessing
print("Old model training data stats:", train_df['severity'].describe())
print("New model training data stats:", covid_df['severity'].describe())

# Try unfreezing layers in EfficientNet + fine-tuning
base_model = new_severity_model.layers[0]
base_model.trainable = True  # Unfreeze
new_severity_model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='mse', metrics=['mae'])
history = new_severity_model.fit(train_gen, validation_data=val_gen, epochs=10)

def predict_severity(image_path):
    # Step 1: Binary classification (COVID or not?)
    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))
    img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0
    is_covid = binary_model.predict(np.expand_dims(img_array, axis=0))[0][0] > 0.5

    if not is_covid:
        return "Non-COVID (No severity score)"

    # Step 2: Predict severity if COVID
    severity = severity_model.predict(np.expand_dims(img_array, axis=0))[0][0]
    return f"COVID-19 | Predicted Severity: {severity:.2f}%"

# Test on a sample image
sample_image = covid_df.iloc[0]['original_path']  # Replace with your image path
print(predict_severity(sample_image))

# Save all models and metadata
import pickle

pipeline = {
    'binary_model': binary_model,
    'severity_model': severity_model,
    'class_labels': ['Non-COVID', 'COVID']
}

with open('/content/drive/MyDrive/covid_severity_pipeline.pkl', 'wb') as f:
    pickle.dump(pipeline, f)

# Plot training history (if available)
plt.plot(history.history['mae'], label='Train MAE')
plt.plot(history.history['val_mae'], label='Validation MAE')
plt.title('Old Model (DenseNet121) Training Progress')
plt.xlabel('Epochs')
plt.ylabel('MAE (%)')
plt.legend()
plt.show()

# Visualize predictions vs. ground truth
test_gen.reset()
y_pred = severity_model.predict(test_gen).flatten()
y_true = test_gen.labels

plt.scatter(y_true, y_pred, alpha=0.5)
plt.plot([0, 100], [0, 100], 'r--')
plt.title(f'Old Model Predictions (MAE: 4.73%)')
plt.xlabel('True Severity (%)')
plt.ylabel('Predicted Severity (%)')
plt.show()

plt.plot(history.history['mae'], label='Train MAE')
plt.plot(history.history['val_mae'], label='Validation MAE')
plt.title('New Model (EfficientNetB0) Training Progress')
plt.xlabel('Epochs')
plt.ylabel('MAE (%)')
plt.legend()
plt.show()

plt.scatter(y_true, y_pred, alpha=0.5)
plt.plot([0, 100], [0, 100], 'r--')
plt.title(f'New Model Predictions (MAE: 28.95%)')
plt.xlabel('True Severity (%)')
plt.ylabel('Predicted Severity (%)')
plt.show()

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import LearningRateScheduler

# Unfreeze some layers for fine-tuning
for layer in severity_model.layers[0].layers[-10:]:
    layer.trainable = True

# Adjust learning rate dynamically
def lr_scheduler(epoch, lr):
    return lr * 0.9 if epoch % 5 == 0 else lr

severity_model.compile(
    optimizer=Adam(1e-5),
    loss='mse',
    metrics=['mae']
)

history = severity_model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=30,
    callbacks=[
        LearningRateScheduler(lr_scheduler),
        EarlyStopping(patience=5)
    ]
)

# Evaluate on test set
test_results = severity_model.evaluate(test_gen)
print(f"Test MSE: {test_results[0]:.4f}, Test MAE: {test_results[1]:.4f}")

# Compare with original model's MAE (4.73%)
if test_results[1] < 4.73:
    print("Improvement achieved! New MAE is better than original 4.73%")
else:
    print("No improvement. Consider alternative approaches.")

# Plot training history
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['mae'], label='Train MAE')
plt.plot(history.history['val_mae'], label='Validation MAE')
plt.title('MAE Progress')
plt.ylabel('MAE')
plt.xlabel('Epoch')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Progress')
plt.ylabel('MSE Loss')
plt.xlabel('Epoch')
plt.legend()
plt.show()

# Visualize predictions vs ground truth
test_gen.reset()
y_pred = severity_model.predict(test_gen).flatten()
y_true = test_gen.labels

plt.figure(figsize=(8, 6))
plt.scatter(y_true, y_pred, alpha=0.5)
plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], 'r--')
plt.xlabel('True Severity (%)')
plt.ylabel('Predicted Severity (%)')
plt.title(f'Severity Predictions (MAE: {test_results[1]:.2f}%)')
plt.show()

# Save the improved model
severity_model.save('/content/drive/MyDrive/improved_severity_model.h5')

# Proceed to build end-to-end pipeline
def predict_severity(image_path):
    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))
    img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0
    severity = severity_model.predict(np.expand_dims(img_array, axis=0))[0][0]
    return severity

print("Sample prediction:", predict_severity(test_df.iloc[0]['original_path']))

severity_model.save('/content/drive/MyDrive/improved_severity_model.keras')  # Preferred format

import numpy as np
from tensorflow.keras.preprocessing import image

def predict_covid_severity(image_path, covid_threshold=0.5):
    """Pipeline: COVID detection → Severity prediction"""
    # Load and preprocess image
    img = image.load_img(image_path, target_size=(224, 224))
    img_array = image.img_to_array(img) / 255.0
    img_batch = np.expand_dims(img_array, axis=0)

    # Step 1: COVID detection
    is_covid = binary_model.predict(img_batch)[0][0] > covid_threshold

    if not is_covid:
        return {"status": "Non-COVID", "severity": None}

    # Step 2: Severity prediction
    severity = severity_model.predict(img_batch)[0][0]
    return {
        "status": "COVID-19",
        "severity": float(severity),
        "severity_category": classify_severity(severity)  # Optional
    }

def classify_severity(score):
    """Convert severity % to category"""
    if score < 10: return "Mild"
    elif score < 25: return "Moderate"
    elif score < 50: return "Severe"
    else: return "Critical"

# Test on a sample
print(predict_covid_severity(test_df.iloc[0]['original_path']))

!pip install gradio
import gradio as gr

def gradio_predict(image):
    result = predict_covid_severity(image.name)  # image.name gives temp file path
    if result["status"] == "Non-COVID":
        return "Non-COVID (No severity score)"
    return f"COVID-19 | Severity: {result['severity']:.2f}% ({result['severity_category']})"

gr.Interface(
    fn=gradio_predict,
    inputs=gr.Image(type="filepath"),
    outputs="text",
    title="COVID-19 Severity Predictor",
    description="Upload a chest X-ray to assess COVID-19 severity"
).launch()

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image

# Get 10 random test samples
test_samples = test_df.sample(10, random_state=42)

plt.figure(figsize=(20, 10))
for i, (_, row) in enumerate(test_samples.iterrows(), 1):
    # Load and preprocess image
    img = image.load_img(row['original_path'], target_size=(224, 224))
    img_array = image.img_to_array(img) / 255.0

    # Predict
    severity_true = row['severity']
    severity_pred = severity_model.predict(np.expand_dims(img_array, axis=0), verbose=0)[0][0]

    # Plot
    plt.subplot(2, 5, i)
    plt.imshow(img)
    plt.title(f"True: {severity_true:.1f}%\nPred: {severity_pred:.1f}%")
    plt.axis('off')
plt.tight_layout()
plt.show()

from sklearn.metrics import r2_score, mean_absolute_error

# Reset and get all test predictions
test_gen.reset()
y_true = test_gen.labels
y_pred = severity_model.predict(test_gen, verbose=1).flatten()

# Ensure equal length
y_true = y_true[:len(y_pred)] if len(y_true) > len(y_pred) else y_true

# Key Metrics
print(f"""
Regression Metrics:
- MAE: {mean_absolute_error(y_true, y_pred):.2f}%
- R² Score: {r2_score(y_true, y_pred):.2f}
- Error Stats:
  - Mean Error: {np.mean(y_true - y_pred):.2f}%
  - Std Dev: {np.std(y_true - y_pred):.2f}%
  - Max Overestimation: {np.min(y_true - y_pred):.2f}%
  - Max Underestimation: {np.max(y_true - y_pred):.2f}%
""")

# Error distribution plot
plt.figure(figsize=(10,5))
sns.histplot(y_true - y_pred, bins=20, kde=True)
plt.title('Prediction Error Distribution')
plt.xlabel('True - Predicted Severity (%)')
plt.show()

